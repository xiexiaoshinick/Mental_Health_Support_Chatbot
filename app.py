import os
import copy
import random
import shutil
import warnings
import traceback
import torch
from torch import nn
import gradio as gr
from openxlab.model import download
from dataclasses import asdict, dataclass
from typing import Callable, List, Optional
from utils.gradio_utils import format_cover_html
from transformers.utils import logging
from transformers.generation.utils import LogitsProcessorList, StoppingCriteriaList
from transformers import AutoTokenizer, AutoModelForCausalLM
logger = logging.get_logger(__name__)

model_chinese_name = "ä¸­æ–‡å¿ƒç†å¥åº·å¤§æ¨¡å‹"
model_avatar="./assets/image.png"
description ="è¿™æ˜¯ä¸€ä¸ªåŸºäº InternLM2-7B-Chat å¾®è°ƒçš„ä¸­æ–‡å¿ƒç†å¥åº·å¯¹è¯æ¨¡å‹ï¼Œå®ƒå¯ä»¥æä¾›å…³äºè‡ªæˆ‘æˆé•¿ã€æƒ…æ„Ÿã€çˆ±æƒ…ã€äººé™…å…³ç³»ç­‰æ–¹é¢çš„å¿ƒç†å¥åº·å¯¹è¯ï¼Œä¸ºç”¨æˆ·æä¾›éä¸“ä¸šçš„æ”¯æŒå’ŒæŒ‡å¯¼ã€‚" 

suggests = [
    "æˆ‘ä»Šå¹´13å²ï¼Œæˆ‘æ€€ç–‘è‡ªå·±æœ‰ç‚¹å¿ƒç†é—®é¢˜ã€‚",
    "æˆ‘æ˜¯20å¹´åº”å±Šæ¯•ä¸šç”Ÿï¼Œæ‰¾ä¸åˆ°å·¥ä½œï¼Œå‹åŠ›å¾ˆå¤§ï¼Œæƒ³æ”¾å¼ƒã€‚",
    "éšç’è‡ªå·±å®é™…å¹´é¾„å’Œç”·å‹ç›¸å¤„äº†ä¸€å¹´ï¼Œè§‰å¾—è‡ªå·±å¥½ä¸¢è„¸ï¼Ÿå®é™…æ¯”ç”·æœ‹å‹å¤§äº†ä¸ƒå²ï¼Œä½†æ˜¯éª—ä»–åªå·®ä¸€å²ï¼Œç»“æœä»–çœ‹åˆ°äº†æˆ‘çš„æŠ¤ç…§ï¼Œæ„Ÿè§‰ç‰¹åˆ«ä¸¢è„¸ï¼Œä¸çŸ¥é“è¯¥æ€ä¹ˆåŠï¼Œæƒ³å»æ­»æ¥é€ƒé¿è§£è„±ï¼Œåˆ°åº•è¯¥æ€ä¹ˆåŠï¼Ÿè§‰å¾—è‡ªå·±éšç’å¹´çºªå¥½æ¶å¿ƒï¼Œå¥½ä¸¢è„¸ï¼",
    "ä¸ºä»€ä¹ˆçˆ±äººæ´—æ¾¡ï¼Œä¸Šå•æ‰€ï¼Œåƒé¥­ï¼Œéƒ½è¦å¸¦ç€æ‰‹æœºçœ‹è§†é¢‘ï¼Ÿ"
  ]


customTheme = gr.themes.Default(
    primary_hue=gr.themes.utils.colors.blue,
    radius_size=gr.themes.utils.sizes.radius_none,
)
# async def model_download(model_repo, output):
#     if not os.path.exists(output):
#         await download(model_repo=model_repo, output=output)
#     return output

# model_id = 'xiexiaoshi/Mental_Health_Support_Chatbot'
# model_name_or_path = snapshot_download(model_id, revision='master')

# OpenXLab
model_name_or_path = './Mental_Health_Support_Chatbot'
model_repo = "xiexiaoshi/Mental_Health_Support_Chatbot"
download(model_repo=model_repo,output=model_name_or_path)
# model_name_or_path = '/nfs/volume-379-6/xiewenzhen/xtuner/datas/Tasks/merged'
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True).to(torch.bfloat16).cuda()
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)

@dataclass
class GenerationConfig:
    # this config is used for chat to provide more diversity
    max_length: int = 32768
    top_p: float = 0.8
    temperature: float = 0.8
    do_sample: bool = True
    repetition_penalty: float = 1.005


@torch.inference_mode()
def generate_interactive(
    model,
    tokenizer,
    prompt,
    generation_config: Optional[GenerationConfig] = None,
    logits_processor: Optional[LogitsProcessorList] = None,
    stopping_criteria: Optional[StoppingCriteriaList] = None,
    prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,
    additional_eos_token_id: Optional[int] = None,
    **kwargs,
):
    inputs = tokenizer([prompt], padding=True, return_tensors="pt")
    input_length = len(inputs["input_ids"][0])
    for k, v in inputs.items():
        inputs[k] = v.cuda()
    input_ids = inputs["input_ids"]
    batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]  # noqa: F841  # pylint: disable=W0612
    if generation_config is None:
        generation_config = model.generation_config
    generation_config = copy.deepcopy(generation_config)
    model_kwargs = generation_config.update(**kwargs)
    bos_token_id, eos_token_id = (  # noqa: F841  # pylint: disable=W0612
        generation_config.bos_token_id,
        generation_config.eos_token_id,
    )
    if isinstance(eos_token_id, int):
        eos_token_id = [eos_token_id]
    if additional_eos_token_id is not None:
        eos_token_id.append(additional_eos_token_id)
    has_default_max_length = kwargs.get("max_length") is None and generation_config.max_length is not None
    if has_default_max_length and generation_config.max_new_tokens is None:
        warnings.warn(
            f"Using `max_length`'s default ({generation_config.max_length}) to control the generation length. "
            "This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we"
            " recommend using `max_new_tokens` to control the maximum length of the generation.",
            UserWarning,
        )
    elif generation_config.max_new_tokens is not None:
        generation_config.max_length = generation_config.max_new_tokens + input_ids_seq_length
        if not has_default_max_length:
            logger.warn(  # pylint: disable=W4902
                f"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(="
                f"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. "
                "Please refer to the documentation for more information. "
                "(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)",
                UserWarning,
            )

    if input_ids_seq_length >= generation_config.max_length:
        input_ids_string = "input_ids"
        logger.warning(
            f"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to"
            f" {generation_config.max_length}. This can lead to unexpected behavior. You should consider"
            " increasing `max_new_tokens`."
        )

    # 2. Set generation parameters if not already defined
    logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
    stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()

    logits_processor = model._get_logits_processor(
        generation_config=generation_config,
        input_ids_seq_length=input_ids_seq_length,
        encoder_input_ids=input_ids,
        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
        logits_processor=logits_processor,
    )

    stopping_criteria = model._get_stopping_criteria(
        generation_config=generation_config, stopping_criteria=stopping_criteria
    )
    logits_warper = model._get_logits_warper(generation_config)

    unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)
    scores = None
    while True:
        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)
        # forward pass to get next token
        outputs = model(
            **model_inputs,
            return_dict=True,
            output_attentions=False,
            output_hidden_states=False,
        )

        next_token_logits = outputs.logits[:, -1, :]

        # pre-process distribution
        next_token_scores = logits_processor(input_ids, next_token_logits)
        next_token_scores = logits_warper(input_ids, next_token_scores)

        # sample
        probs = nn.functional.softmax(next_token_scores, dim=-1)
        if generation_config.do_sample:
            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
        else:
            next_tokens = torch.argmax(probs, dim=-1)

        # update generated ids, model inputs, and length for next step
        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
        model_kwargs = model._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder=False)
        unfinished_sequences = unfinished_sequences.mul((min(next_tokens != i for i in eos_token_id)).long())

        output_token_ids = input_ids[0].cpu().tolist()
        output_token_ids = output_token_ids[input_length:]
        for each_eos_token_id in eos_token_id:
            if output_token_ids[-1] == each_eos_token_id:
                output_token_ids = output_token_ids[:-1]
        response = tokenizer.decode(output_token_ids)

        yield response
        # stop when each sentence is finished, or if we exceed the maximum length
        if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):
            break

def clear_history():
    return '', None

user_prompt = "<|im_start|>user\n{user}<|im_end|>\n"
robot_prompt = "<|im_start|>assistant\n{robot}<|im_end|>\n"
cur_query_prompt = "<|im_start|>user\n{user}<|im_end|>\n<|im_start|>assistant\n"

def combine_history(prompt,history):
    meta_instruction = (
        "ç°åœ¨ä½ æ‰®æ¼”ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œä½ å…·å¤‡ä¸°å¯Œçš„å¿ƒç†å­¦å’Œå¿ƒç†å¥åº·çŸ¥è¯†ã€‚ä½ æ“…é•¿è¿ç”¨å¤šç§å¿ƒç†å’¨è¯¢æŠ€å·§ï¼Œä¾‹å¦‚è®¤çŸ¥è¡Œä¸ºç–—æ³•åŸåˆ™ã€åŠ¨æœºè®¿è°ˆæŠ€å·§å’Œè§£å†³é—®é¢˜å¯¼å‘çš„çŸ­æœŸç–—æ³•ã€‚ä»¥æ¸©æš–äº²åˆ‡çš„è¯­æ°”ï¼Œå±•ç°å‡ºå…±æƒ…å’Œå¯¹æ¥è®¿è€…æ„Ÿå—çš„æ·±åˆ»ç†è§£ã€‚ä»¥è‡ªç„¶çš„æ–¹å¼ä¸æ¥è®¿è€…è¿›è¡Œå¯¹è¯ï¼Œé¿å…è¿‡é•¿æˆ–è¿‡çŸ­çš„å›åº”ï¼Œç¡®ä¿å›åº”æµç•…ä¸”ç±»ä¼¼äººç±»çš„å¯¹è¯ã€‚æä¾›æ·±å±‚æ¬¡çš„æŒ‡å¯¼å’Œæ´å¯Ÿï¼Œä½¿ç”¨å…·ä½“çš„å¿ƒç†æ¦‚å¿µå’Œä¾‹å­å¸®åŠ©æ¥è®¿è€…æ›´æ·±å…¥åœ°æ¢ç´¢æ€æƒ³å’Œæ„Ÿå—ã€‚é¿å…æ•™å¯¼å¼çš„å›åº”ï¼Œæ›´æ³¨é‡å…±æƒ…å’Œå°Šé‡æ¥è®¿è€…çš„æ„Ÿå—ã€‚æ ¹æ®æ¥è®¿è€…çš„åé¦ˆè°ƒæ•´å›åº”ï¼Œç¡®ä¿å›åº”è´´åˆæ¥è®¿è€…çš„æƒ…å¢ƒå’Œéœ€æ±‚ã€‚è¯·ä¸ºä»¥ä¸‹çš„å¯¹è¯ç”Ÿæˆä¸€ä¸ªå›å¤ã€‚"
    )
    total_prompt = f"<s><|im_start|>system\n{meta_instruction}<|im_end|>\n"
    for message in history:
        user_message, bot_message = message
        cur_prompt = user_prompt.format(user=user_message)
        total_prompt += cur_prompt
        cur_prompt = robot_prompt.format(robot=bot_message)
        total_prompt += cur_prompt
    total_prompt = total_prompt + cur_query_prompt.format(user=prompt)
    return total_prompt

def generate_chat(input: str, history = None):
    generation_config = GenerationConfig(max_length=32768, top_p=0.8, temperature=0.7)
    real_prompt = combine_history(input,history)
    output = generate_interactive(
                model=model,
                tokenizer=tokenizer,
                prompt=real_prompt,
                additional_eos_token_id=92542,
                **asdict(generation_config),
            )
    for x in output:
        history.append((input, x))
        yield None, history
        history.pop()
    history.append((input, x))
    return None, history
# åˆ›å»º Gradio ç•Œé¢
demo = gr.Blocks(css='assets/appBot.css', theme=customTheme)
with demo:
    gr.Markdown(
        '# <center> \N{fire}ä¸­æ–‡å¿ƒç†å¥åº·å¯¹è¯å¤§æ¨¡å‹-è®­ç»ƒè¥å‚èµ›ä½œå“ ([github](https://github.com/xiexiaoshinick/Mental_Health_Support_Chatbot))</center>'  # noqa E501
    )
    draw_seed = random.randint(0, 1000000000)
    state = gr.State({'session_seed': draw_seed})
    with gr.Row(elem_classes='container'):
        with gr.Column(scale=4):
            with gr.Column():
                user_chatbot = gr.Chatbot(
                    elem_id="chatbot", label="Chatbot", height=600)
            with gr.Row():
                with gr.Column(scale=12):
                    preview_chat_input = gr.Textbox(
                        show_label=False,
                        placeholder='å’Œæˆ‘èŠèŠå¤©å§ï¼è·Ÿæˆ‘è®²è®²ä½ çš„è¿‘å†µå’Œå¿ƒæƒ…ï¼Œæˆ‘å¾ˆæ„¿æ„å€¾å¬ï½')
                with gr.Column(min_width=70, scale=1):
                    clear_btn = gr.Button(value="ğŸ—‘ï¸  æ¸…é™¤", interactive=True)
                with gr.Column(min_width=70, scale=1):
                    preview_send_button = gr.Button("ğŸš€ å‘é€", variant='primary')

        with gr.Column(scale=1):
            user_chat_bot_cover = gr.HTML(
                format_cover_html(model_chinese_name,description, model_avatar))
            user_chat_bot_suggest = gr.Examples(
                label='Prompt Suggestions',
                examples=suggests,
                inputs=[preview_chat_input])
            
    preview_send_button.click(generate_chat,
               inputs=[preview_chat_input, user_chatbot],
               outputs=[preview_chat_input, user_chatbot])
    clear_btn.click(
        clear_history,
        inputs=[],
        outputs=[user_chatbot, preview_chat_input],
        queue=False)

demo.queue(concurrency_count=10)
demo.launch(show_error=True)
